## 爬虫

```html
	明确目标（确定在哪个网站搜索）
	爬（爬下内容）
	取（筛选想要的）
	处理数据（按照你的想法去处理）
```

## 正则

```hyml
则表达式
	文档：https://studygolang.com/pkgdoc
	API
	re := regexp.MustCompile(reStr)，传入正则表达式，得到正则表达式对象
	ret := re.FindAllStringSubmatch(srcStr,-1)：用正则对象，获取页面页面，srcStr是页面内容，-1代表取全部
	爬邮箱
	方法抽取
	爬超链接
	爬手机号
	http://www.zhaohaowang.com/
	爬身份证号
	http://henan.qq.com/a/20171107/069413.htm
	爬图片链接
```

## 爬取页面中的邮箱

```go
package main

import (
	"fmt"
	"io/ioutil"
	"net/http"
	"regexp"
)
var (
	reQQemail=`(\d+@qq.com)`
	email = `[a-zA-z0-9]+@`
)

// 爬取邮箱
func GetEmail(url string) (ret interface{}) {
	resp, err := http.Get(url)
	HandleError(err, "http.Get url")
	defer resp.Body.Close()
	// 读取页面内容
	pageBytes, err := ioutil.ReadAll(resp.Body) // readall 读数据流 字节数据
	HandleError(err, "ioutil.ReadAll")
	// 字节转换字符串
	pageStr := string(pageBytes)
	// 过滤数据(过滤QQ邮箱)
	re := regexp.MustCompile(reQQemail)
	ret = re.FindAllStringSubmatch(pageStr, -1)// -1 表示匹配全部
	return ret

}

func HandleError(err error, s string) {
	if err != nil{
		fmt.Println(err, s)
	}
}

func main()  {
	url := "https://tieba.baidu.com/p/6685575928"
	ret := GetEmail(url)
	fmt.Println("ret", ret)
}
```

## 并发的爬取图片并完成下载

```go
package main

import (
	"fmt"
	"io/ioutil"
	"net/http"
	"regexp"
	"strconv"
	"strings"
	"sync"
	"time"
)

var (
	reEmail = ``
	//reImage=`"https?://[^"]+?(\.((jpg)|(png)))"`
	reImage = `https?://[^"]+?(\.((jpg)|(png)))`

	// 声明存放图片连接的管道
	chanImageUrls chan string
	// 声明等待组
	waitGroup sync.WaitGroup
	// 声明任务是否完成的状态管道 用户监控协程
	chanTaskResult chan string
)

func HandleError(err error, s string) {
	if err != nil {
		fmt.Println(err, s)
	}
}

func main() {
	//url := "https://www.umei.cc/p/gaoqing/rihan/4.htm"
	//GetSpider(url, reImage)
	//DownloadImg("https://www.umei.cc/p/gaoqing/rihan/20160621185501.htm", "1.jpeg")
	// 1初始化管道
	chanImageUrls = make(chan string, 1000000) //缓冲 1000000
	chanTaskResult = make(chan string, 26)     // 开启26个协程
	// 2爬虫协程
	for i := 1; i < 27; i++ {
		waitGroup.Add(1)
		go getImgUrls("https://www.umei.cc/p/gaoqing/rihan/" + strconv.Itoa(i) + ".htm")
	}
	// 3 任务统计协程，统计26个任务是否都完成，完成则关闭管道
	waitGroup.Add(1)
	go CheckOK()

	// 4 下载协程，从管道中读取链接并下载图片
	for i := 0; i < 5; i++ {
		waitGroup.Add(1)
		go DownloadImage()
	}
	waitGroup.Done()
	fmt.Println("done")
	//for  {
	//
	//}

}

// 爬图片链接到管道
// url是页面整体的url
func getImgUrls(url string) {
	urls := getImages(url) // 切片
	fmt.Println("urls==", urls)
	// 遍历切片里所有链接，存入数据管道
	for _, url := range urls {
		chanImageUrls <- url
	}
	// 标识当前协程完成
	// 每完成一个任务写一条数据
	// 用户监控协程知道已经完成几个任务
	chanImageUrls <- url
	waitGroup.Done()

}

// 获取当前页的图片链接
func getImages(url string) (urls []string) {
	pageStr := GetPageStr(url)
	re := regexp.MustCompile(reImage)
	results := re.FindAllStringSubmatch(pageStr, -1)
	fmt.Printf("共%d张图片\n", len(results))
	for _, value := range results {
		urlSingle := value[0]
		urls = append(urls, urlSingle)
	}
	return
}

// 图片下载
func DownloadImage() {
	for url := range chanImageUrls {
		filename := GetFilenameFromUrl(url)
		ok := DownloadFile(url, filename)
		if ok{
			fmt.Printf("%s 下载成功！" , filename)
		}else {
			fmt.Printf("%s 下载失败！" , filename)
		}
	}
	waitGroup.Done()
}

// 文件下载
func DownloadFile(url string, filename string) (ok bool) {
	resp, err := http.Get(url)
	HandleError(err, "http.Get url")
	defer resp.Body.Close()
	bytes, err := ioutil.ReadAll(resp.Body)
	HandleError(err, "resp.body")
	filename = "/Users/mosson/Documents/Learn/goLearn/goProject/src/awesomeProject/pa_email/img/" + filename
	fmt.Println(filename)
	// 写数据
	err = ioutil.WriteFile(filename, bytes, 0666)
	if err != nil {
		fmt.Println(err)
		return false
	} else {
		return true
	}

}

// 截取url名字
func GetFilenameFromUrl(url string) (filename string) {
	// 返回最后一个 / 的位置
	lastIndex := strings.LastIndex(url, "/")
	// 切出来
	filename = url[lastIndex+1:]
	// 利用时间戳解决重名问题
	timePrefix := strconv.Itoa(int(time.Now().Unix()))
	filename = timePrefix + "_" + filename
	return
}

// 任务统计
func CheckOK() {
	var count int
	for {
		url := <-chanTaskResult
		fmt.Printf("%s完成爬取任务\n", url)
		if count == 26 {
			close(chanImageUrls)
			break
		}
	}
	waitGroup.Done()
}


func GetPageStr(url string) (pageStr string) {
	resp, err := http.Get(url)
	HandleError(err, "http.Get url")
	defer resp.Body.Close()
	// 读取页面内容
	pageBytes, err := ioutil.ReadAll(resp.Body) // readall 读数据流 字节数据
	HandleError(err, "ioutil.ReadAll")
	// 字节转换字符串
	pageStr = string(pageBytes)
	return pageStr
}
```

